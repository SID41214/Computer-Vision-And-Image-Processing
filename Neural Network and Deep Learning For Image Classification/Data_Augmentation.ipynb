{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "zkUQVW5zwDrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train a Convolutional Neural Network with Regular data and Augmented data. The purpose of this lab is to show that the Augmented data improves generalization performance.\n",
        "\n",
        "* Think of a scenario where a drone has to take a picture of an object. The drone is moving and the object can also possibly be moving. When an image is taken we arent always going to get perfect images. The subject may not be perfectly centered in the image or the subject may be rotated in the image. In this case, a model trained on perfectly centered or rotated images won't perform well. This is why we train a model on rotated data so it can perform well on imperfect images.\n",
        "\n",
        "* In this assignment, we will use a dataset of digit images. We will have two models one trained on non rotated digits and one trained on rotated images and then we will test the models on a rotated testing dataset which will be more realistic and robust in terms of our scenario above.\n",
        "\n",
        "* Get Some Data\n",
        "* Convolutional Neural Network\n",
        "* Rotated Training Data"
      ],
      "metadata": {
        "id": "aWevzxcxbczw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "owtPzoHDbp7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b1brYUYOv23c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ed44a5-0aad-402a-ea0d-cbb4f8d3bc98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a Pretrained Model because training takes a long time\n",
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/normal.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlS7SwpJeMV6",
        "outputId": "1fbac7e7-3eb4-4f3b-94a0-004eceac3bc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-28 04:26:59--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/normal.pt\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75134 (73K) [application/octet-stream]\n",
            "Saving to: ‘normal.pt.6’\n",
            "\n",
            "\rnormal.pt.6           0%[                    ]       0  --.-KB/s               \rnormal.pt.6         100%[===================>]  73.37K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-12-28 04:26:59 (24.8 MB/s) - ‘normal.pt.6’ saved [75134/75134]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a Pretrained Model Trained on Augmented Data because training takes a long time\n",
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/rotated_data.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gek35lMJeVAT",
        "outputId": "aa51f197-a965-4a7d-dd1a-7203fd2883a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-28 04:26:59--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/rotated_data.pt\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75134 (73K) [application/octet-stream]\n",
            "Saving to: ‘rotated_data.pt.6’\n",
            "\n",
            "\rrotated_data.pt.6     0%[                    ]       0  --.-KB/s               \rrotated_data.pt.6   100%[===================>]  73.37K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2024-12-28 04:26:59 (19.7 MB/s) - ‘rotated_data.pt.6’ saved [75134/75134]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Library to Show Images\n",
        "!pip install Pillow==6.2.2\n",
        "# !pip install pillow==9.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg_Y36Y8faQP",
        "outputId": "fb52fbf4-983f-4bba-f83f-30b678abe7a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow==6.2.2 in /usr/local/lib/python3.10/dist-packages (6.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pillow"
      ],
      "metadata": {
        "id": "BlN7NscqgWcJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the kernel**"
      ],
      "metadata": {
        "id": "R7RnwRz5fw5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries we need to use in this lab\n",
        "# Using the following line code to install the torchvision library\n",
        "# !conda install -y torchvision\n",
        "\n",
        "# PyTorch Library\n",
        "import torch\n",
        "# PyTorch Neural Network Library\n",
        "import torch.nn as nn\n",
        "# Allows us to transform data\n",
        "import torchvision.transforms as transforms\n",
        "# Used to graph data and loss curves\n",
        "import matplotlib.pylab as plt\n",
        "# Allows us to use arrays to manipulate and store data\n",
        "import numpy as np\n",
        "# Allows us to download the dataset\n",
        "import torchvision.datasets as dsets\n",
        "# Allows us to access the filesystem\n",
        "import os"
      ],
      "metadata": {
        "id": "ZXYnf3SggL8p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Cost and Accuracy vs Epoch Graph"
      ],
      "metadata": {
        "id": "KDCmath1g3S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cost_accuracy(checkpoint):\n",
        "\n",
        "# Plot the cost and accuracy\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    color = 'tab:red'\n",
        "    ax1.plot(checkpoint['cost'], color=color)\n",
        "    ax1.set_xlabel('epoch', color=color)\n",
        "    ax1.set_ylabel('Cost', color=color)\n",
        "    ax1.tick_params(axis='y', color=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('accuracy', color=color)\n",
        "    ax2.set_xlabel('epoch', color=color)\n",
        "    ax2.plot( checkpoint['accuracy'], color=color)\n",
        "    ax2.tick_params(axis='y', color=color)\n",
        "    fig.tight_layout()"
      ],
      "metadata": {
        "id": "Xwb-FRnTg3tZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function show_data to plot out data samples as images."
      ],
      "metadata": {
        "id": "AE_FJBO2hFZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_data(data_sample):\n",
        "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
        "    plt.title('y = '+ str(data_sample[1]))"
      ],
      "metadata": {
        "id": "KFEivvkMhFvk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot first 5 misclassified samples"
      ],
      "metadata": {
        "id": "SVTu1gkxhHYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mis_classified(model, dataset):\n",
        "    count=0\n",
        "    for x, y in torch.utils.data.DataLoader(dataset=dataset, batch_size=1):\n",
        "        z = model(x)\n",
        "        _, yhat = torch.max(z, 1)\n",
        "        if yhat != y:\n",
        "            show_data((x, y))\n",
        "            plt.show()\n",
        "            count += 1\n",
        "        if count >= 5:\n",
        "            break"
      ],
      "metadata": {
        "id": "Er-d6XFphIqd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "pgGa0wIyhU7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a transform object compose one will resize the image and convert it to a tensor, the second will also rotate the image Randomly rotate the image."
      ],
      "metadata": {
        "id": "y-ekvFnihVxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the images are 16 by 16\n",
        "IMAGE_SIZE = 16\n",
        "\n",
        "# Creating a group of transformations to created a rotated dataset\n",
        "# Resizes the images, randomly rotates it, and then converts it to a tensor\n",
        "compose_rotate = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),transforms.RandomAffine(45), transforms.ToTensor()])\n",
        "\n",
        "# Creating a group of transformations to created a non rotated dataset\n",
        "# Resizes the images then converts it to a tensor\n",
        "compose = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "5CvH0TRthXRc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the training dataset by setting the parameters train to True. We use the transform defined above, one with rotated data one without."
      ],
      "metadata": {
        "id": "_FcauIYOhaZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The transform parameters is set to the corresponding compose\n",
        "train_dataset_rotate = dsets.MNIST(root='./data', train=True, download=True, transform=compose_rotate)\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=compose)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVCeKm54hazc",
        "outputId": "d1b1c769-fccf-438b-b3a8-9f7cabcea381"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 48.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 35.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 120MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.45MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the testing dataset by setting the parameters train to False, where the data is ALL rotated."
      ],
      "metadata": {
        "id": "j4EI9F48hdzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the testing dataset\n",
        "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=compose_rotate)"
      ],
      "metadata": {
        "id": "nhy4hfcUheLV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.\n",
        "\n",
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">"
      ],
      "metadata": {
        "id": "q6CTCjAYhz03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the first sample"
      ],
      "metadata": {
        "id": "TzGF5UMnh3EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The image for the first data sample\n",
        "show_data(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Rb5y9TmQh6Ak",
        "outputId": "dec19945-6157-405b-e1f4-2c5ef69af69e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the first label, as we can see this digit is a five"
      ],
      "metadata": {
        "id": "UJ8nFNIQh76Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The label for the first data element\n",
        "train_dataset[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyGMPiYnh8M2",
        "outputId": "9c65c238-e649-4874-f7cc-775595132c56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the first sample of the rotated training dataset"
      ],
      "metadata": {
        "id": "nQboT_3eh-uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_data(train_dataset_rotate[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "w6XChpdzh9hW",
        "outputId": "cdcfaf99-fba1-474e-bb49-35e45caf6ca9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this is the same sample but it is rotated as we wanted"
      ],
      "metadata": {
        "id": "RbsEd1bxiBxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Convolutional Neural Network Class\n",
        "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layers."
      ],
      "metadata": {
        "id": "aQlsnlKtiDji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    # Contructor\n",
        "    def __init__(self, out_1=16, out_2=32):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # The reason we start with 1 channel is because we have a single black and white image\n",
        "        # Channel Width after this layer is 16\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
        "        # Channel Wifth after this layer is 8\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Channel Width after this layer is 8\n",
        "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
        "        # Channel Width after this layer is 4\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
        "        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n",
        "        # The output is a value for each class\n",
        "        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n",
        "\n",
        "    # Prediction\n",
        "    def forward(self, x):\n",
        "        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n",
        "        x = self.cnn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.cnn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "    # Outputs result of each stage of the CNN, relu, and pooling layers\n",
        "    def activations(self, x):\n",
        "        # Outputs activation this is not necessary\n",
        "        z1 = self.cnn1(x)\n",
        "        a1 = torch.relu(z1)\n",
        "        out = self.maxpool1(a1)\n",
        "\n",
        "        z2 = self.cnn2(out)\n",
        "        a2 = torch.relu(z2)\n",
        "        out1 = self.maxpool2(a2)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        return z1, a1, z2, a2, out1,out"
      ],
      "metadata": {
        "id": "LQ7AuYkbiGOe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regular Data"
      ],
      "metadata": {
        "id": "Lds_IzJdiJxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Convolutional Neural Network Classifier, Criterion function, Optimizer, and Train the Model"
      ],
      "metadata": {
        "id": "kk1lB9vFigqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model object to be trained on regular data using CNN class\n",
        "model = CNN(out_1=16, out_2=32)"
      ],
      "metadata": {
        "id": "0fkkkdCfih4U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the loss function, the optimizer, and the dataset loader"
      ],
      "metadata": {
        "id": "IkR663IHik5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a criterion which will measure loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "# Create an optimizer that updates model parameters using the learning rate and gradient\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "# Create a Data Loader for the training data with a batch size of 100\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "# Create a Data Loader for the rotated validation data with a batch size of 5000\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
      ],
      "metadata": {
        "id": "TV9Z_5zJilRN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell will train the model, we will comment it out as it takes a long time to run. You can change the block type from Raw to Code and run it or you can load the trained model in the next cell. Notice that we are not only training and saving the model here but we are also keeping track of important data like the cost and accuracy throughout the training process."
      ],
      "metadata": {
        "id": "TJvs4icginr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "import os  \n",
        "\n",
        "# Location to save data\n",
        "file_normal = os.path.join(os.getcwd(), 'normal.pt')\n",
        "\n",
        "# All the data we are saving\n",
        "checkpoint={\n",
        "            # Saving the number of epochs the models was trained for\n",
        "            'epoch': None,\n",
        "            # Saving the models parameters which will allow us to recreate the trained model\n",
        "            'model_state_dict': None,\n",
        "            # Saving the optimizers parameters\n",
        "            'optimizer_state_dict': None,\n",
        "            # Saving the loss on the training dataset for the last batch of the last epoch\n",
        "            'loss': None,\n",
        "            # Saving the cost on the training dataset for each epoch\n",
        "            'cost': [],\n",
        "            # Saving the accuracy for the testing dataset for each epoch\n",
        "            'accuracy': []}\n",
        "            \n",
        "# Number of epochs to train model\n",
        "n_epochs = 5\n",
        "\n",
        "# Size of the testing dataset\n",
        "N_test = len(validation_dataset)\n",
        "\n",
        "# Training for the number of epochs we want\n",
        "for epoch in range(n_epochs):\n",
        "    # Variable to keep track of cost for each epoch\n",
        "    cost = 0\n",
        "    # For each batch in the training dataset\n",
        "    for x, y in train_loader:\n",
        "        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
        "        optimizer.zero_grad()\n",
        "        # Makes a prediction on the image\n",
        "        z = model(x)\n",
        "        # Calculate the loss between the prediction and actual class\n",
        "        loss = criterion(z, y)\n",
        "        # Calculates the gradient value with respect to each weight and bias\n",
        "        loss.backward()\n",
        "        # Updates the weight and bias according to calculated gradient value\n",
        "        optimizer.step()\n",
        "      \n",
        "        # Saves the number of epochs we trained for  \n",
        "        checkpoint['epochs'] = n_epochs\n",
        "        # Saves the models parameters\n",
        "        checkpoint['model_state_dict'] = model.state_dict()\n",
        "        # Saves the optimizers paramters\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "        # Saves the loss for the last batch so ultimately this will be the loss for the last batch of the last epoch\n",
        "        checkpoint['loss'] = loss\n",
        "        # Accumulates the loss\n",
        "        cost += loss.item()\n",
        "        \n",
        "     \n",
        "    # Counter for the correct number of predictions        \n",
        "    correct = 0\n",
        "        \n",
        "    # For each batch in the validation dataset\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # Make a prediction\n",
        "        z = model(x_test)\n",
        "        # Get the class that has the maximum value\n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "        # Counts the number of correct predictions made\n",
        "        correct += (yhat == y_test).sum().item()\n",
        "\n",
        "    accuracy = correct / N_test\n",
        "    print(accuracy)\n",
        "    # Appends the cost of the epoch to a list\n",
        "    checkpoint['cost'].append(cost)\n",
        "    # Appends the accuracy of the epoch to a list\n",
        "    checkpoint['accuracy'].append(accuracy)\n",
        "    # Saves the data in checkpoint to the file location\n",
        "    torch.save(checkpoint, file_normal)"
      ],
      "metadata": {
        "id": "xFbQOUWLiqE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable to keep track of cost for each epoch\n",
        "cost = 0\n",
        "# For each batch in the training dataset\n",
        "for x, y in train_loader:\n",
        "    # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
        "    optimizer.zero_grad()\n",
        "    # Makes a prediction on the image\n",
        "    z = model(x)\n",
        "    # Calculate the loss between the prediction and actual class\n",
        "    loss = criterion(z, y)\n",
        "    # Calculates the gradient value with respect to each weight and bias\n",
        "    loss.backward()\n",
        "    # Updates the weight and bias according to calculated gradient value\n",
        "    optimizer.step()\n",
        "\n",
        "    # Saves the number of epochs we trained for\n",
        "    checkpoint['epochs'] = n_epochs\n",
        "    # Saves the models parameters\n",
        "    checkpoint['model_state_dict'] = model.state_dict()\n",
        "    # Saves the optimizers paramters\n",
        "    checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "    # Saves the loss for the last batch so ultimately this will be the loss for the last batch of the last epoch\n",
        "    checkpoint['loss'] = loss\n",
        "    # Accumulates the loss\n",
        "    cost += loss.item()\n",
        "\n",
        "\n",
        "# Counter for the correct number of predictions\n",
        "correct = 0\n",
        "\n",
        "# For each batch in the validation dataset\n",
        "for x_test, y_test in validation_loader:\n",
        "    # Make a prediction\n",
        "    z = model(x_test)\n",
        "    # Get the class that has the maximum value\n",
        "    _, yhat = torch.max(z.data, 1)\n",
        "    # Counts the number of correct predictions made\n",
        "    correct += (yhat == y_test).sum().item()\n",
        "\n",
        "accuracy = correct / N_test\n",
        "print(accuracy)\n",
        "# Appends the cost of the epoch to a list\n",
        "checkpoint['cost'].append(cost)\n",
        "# Appends the accuracy of the epoch to a list\n",
        "checkpoint['accuracy'].append(accuracy)\n",
        "# Saves the data in checkpoint to the file location\n",
        "torch.save(checkpoint, file_normal)"
      ],
      "metadata": {
        "id": "dNfF9Di8jGnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze Results**"
      ],
      "metadata": {
        "id": "UWBb449virnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the data which is saved in normal.pt"
      ],
      "metadata": {
        "id": "5Mjb52Mwiuvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_normal = torch.load(os.path.join(os.getcwd(),'normal.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWgf85_aixFL",
        "outputId": "cadcfc60-a63e-4d20-80bc-45e3cced663e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-77e3c4ecde14>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_normal = torch.load(os.path.join(os.getcwd(),'normal.pt'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy and Cost vs Epoch Graph"
      ],
      "metadata": {
        "id": "gzRWQnJcizO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the helper function defined at the top and the cost and accuracy lists that we saved\n",
        "plot_cost_accuracy(checkpoint_normal)"
      ],
      "metadata": {
        "id": "H0NJJg8vizqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Five misclassified samples"
      ],
      "metadata": {
        "id": "F-U6-rcDi2ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the model parameters we saved we load them into a model to recreate the trained model\n",
        "model.load_state_dict(checkpoint_normal['model_state_dict'])\n",
        "# Setting the model to evaluation mode\n",
        "model.eval()\n",
        "# Using the helper function plot the first five misclassified samples\n",
        "plot_mis_classified(model,validation_dataset)"
      ],
      "metadata": {
        "id": "aNG7TX_pi2yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rotated Training Data**"
      ],
      "metadata": {
        "id": "uAzEY99fi5dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the loss and accuracy on the validation data:"
      ],
      "metadata": {
        "id": "AXH_R89Mi9As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model object using CNN class\n",
        "model_r = CNN(out_1=16, out_2=32)\n",
        "# We create a criterion which will measure loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "# Create an optimizer that updates model parameters using the learning rate and gradient\n",
        "optimizer = torch.optim.SGD(model_r.parameters(), lr = learning_rate)\n",
        "# Create a Data Loader for the rotated training data with a batch size of 100\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_rotate, batch_size=100)\n",
        "# Create a Data Loader for the rotated validation data with a batch size of 5000\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
      ],
      "metadata": {
        "id": "nlrGYtvYi9Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell will train the model, we will comment it out as it takes a long time to run. You can change the block type from Raw to Code and run it or you can load the trained model in the next cell."
      ],
      "metadata": {
        "id": "4gKm3WWKjAmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Location to save data\n",
        "file_rotated = os.path.join(os.getcwd(), 'rotated_data.pt')\n",
        "\n",
        "# All the data we are saving\n",
        "checkpoint={\n",
        "            # Saving the number of epochs the models was trained for\n",
        "            'epoch': None,\n",
        "            # Saving the models parameters which will allow us to recreate the trained model\n",
        "            'model_state_dict': None,\n",
        "            # Saving the optimizers parameters\n",
        "            'optimizer_state_dict': None,\n",
        "            # Saving the loss on the training dataset for the last batch of the last epoch\n",
        "            'loss': None,\n",
        "            # Saving the cost on the training dataset for each epoch\n",
        "            'cost': [],\n",
        "            # Saving the accuracy for the testing dataset for each epoch\n",
        "            'accuracy': []}\n",
        "\n",
        "# Number of epochs to train model\n",
        "n_epochs = 5\n",
        "\n",
        "# Size of the testing dataset\n",
        "N_test = len(validation_dataset)\n",
        "\n",
        "# Training for the number of epochs we want\n",
        "for epoch in range(n_epochs):\n",
        "    # Variable to keep track of cost for each epoch\n",
        "    cost = 0\n",
        "    # For each batch in the training dataset\n",
        "    for x, y in train_loader:\n",
        "        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
        "        optimizer.zero_grad()\n",
        "        # Makes a prediction on the image\n",
        "        z = model_r(x)\n",
        "        # Calculate the loss between the prediction and actual class\n",
        "        loss = criterion(z, y)\n",
        "        # Calculates the gradient value with respect to each weight and bias\n",
        "        loss.backward()\n",
        "        # Updates the weight and bias according to calculated gradient value\n",
        "        optimizer.step()\n",
        "\n",
        "        # Saves the number of epochs we trained for\n",
        "        checkpoint['epochs'] = n_epochs\n",
        "        # Saves the models parameters\n",
        "        checkpoint['model_state_dict'] = model.state_dict()\n",
        "        # Saves the optimizers paramters\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "        # Saves the loss for the last batch so ultimately this will be the loss for the last batch of the last epoch\n",
        "        checkpoint['loss'] = loss\n",
        "        # Accumulates the loss\n",
        "        cost+=loss.item()\n",
        "\n",
        "\n",
        "    # Counter for the correct number of predictions\n",
        "    correct = 0\n",
        "\n",
        "    # For each batch in the validation dataset\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # Make a prediction\n",
        "        z = model_r(x_test)\n",
        "        # Get the class that has the maximum value\n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "        # Counts the number of correct predictions made\n",
        "        correct += (yhat == y_test).sum().item()\n",
        "\n",
        "    accuracy = correct / N_test\n",
        "    print(accuracy)\n",
        "    # Appends the cost of the epoch to a list\n",
        "    checkpoint['cost'].append(cost)\n",
        "    # Appends the accuracy of the epoch to a list\n",
        "    checkpoint['accuracy'].append(accuracy)\n",
        "    # Saves the data in checkpoint to the file location\n",
        "    torch.save(checkpoint, file_rotated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDaIn9yPjBAO",
        "outputId": "fae52f4d-00ac-482b-cb25-ad1556fb704f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9018\n",
            "0.932\n",
            "0.9394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze Results"
      ],
      "metadata": {
        "id": "WtDuAvOhjQar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the data which is saved in rotated_data.pt"
      ],
      "metadata": {
        "id": "kt1BT26vjSKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_rotated= torch.load(os.path.join(os.getcwd(),'rotated_data.pt'))"
      ],
      "metadata": {
        "id": "m5QaeKcujSj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy and Cost vs Epoch Graph"
      ],
      "metadata": {
        "id": "hMo35czTjUpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the helper function defined at the top and the cost and accuracy lists that we saved\n",
        "plot_cost_accuracy(checkpoint_rotated)"
      ],
      "metadata": {
        "id": "e4U5DfwjjWQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Five misclassified samples"
      ],
      "metadata": {
        "id": "tFF8LZC5jX8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the model parameters we saved we load them into a model to recreate the trained model\n",
        "model_r.load_state_dict(checkpoint_rotated['model_state_dict'])\n",
        "# Setting the model to evaluation mode\n",
        "model.eval()\n",
        "# Using the helper function plot the first five misclassified samples\n",
        "plot_mis_classified(model_r,validation_dataset)"
      ],
      "metadata": {
        "id": "dlllQAzTjYa4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}